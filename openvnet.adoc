= OpenVNetで本格的な仮想ネットワーク
:imagesdir: images/openvnet

仮想ネットワークの機能を、データセンター規模で本格的に利用することを目指して、分散するOpenFlowスイッチ群をOpenVNetを使ってコントロールしてみましょう。

== OpenVNetとは

OpenVNetはOpenFlowで仮想ネットワークを構築するためのフリーソフトウェアです。Tremaを使ってあらゆるパケットの挙動を自由に制御することで、既存のネットワーク上にあたかもユーザ専用のネットワークがあるかのような環境を作り出すことができます。開発はWSF(Wakame Software Foundation)が中心となっており、筆者の一人である山崎の所属する株式会社あくしゅの開発者がメインコミッターを務めています。ソフトウェアライセンスにはLGPLv3を採用し、組織の枠を越えたバザール式の開発がなされています。

OpenVNetは、もともとWSFの中にあった別プロジェクトから、ネットワークに関する技術だけをスピンオフして誕生したものです。スピンオフする元となったプロジェクトでは、Wakame-vdcと言う名前で、2009年からデータセンター全体を仮想化するためのソフトウェアを開発してきました。Wakame-vdcは、データセンタ内部のコンピューティング資源を、動的にマルチテナント化できます。すでにいくつもの企業や研究機関で商業化や実用化が進んでいます。下記に、現時点で公表可能なものだけを挙げます。

- 国立情報学研究所 (NII): 分散処理の実証実験、クラウド教育教材として活用
- 九州電力: 大規模データの分散処理基盤として
- NTT PCコミュニケーションズ: パブリッククラウド WebARENA VPSクラウド
- 京セラコミュニケーションシステム: パブリッククラウド GreenOffice Unified Cloud
- TIS株式会社: Dockerでの活用、クラウドを跨ったL2延伸の共同実証実験

マルチテナントをするためには、仮想化の技術が重要になります。サーバだけでなく、ネットワークも含めて、仮想化を実現しなければなりません。特に後者について、2012年の始めに、Tremaを用いてWakame-vdcへ仮想ネットワークの技術を実装しました。仮想ネットワークの技術だけを利用したいユーザ様のために、この部分の実装が後の2013年の秋にそのまま分離されて、OpenVNetとしてスピンアウト、独立しました。

=== 仮想ネットワークを構築するエッジオーバレイ型アーキテクチャ

OpenVNetによるネットワーク仮想化の特長は「エッジオーバーレイ型」である点です。仮想マシンのような通信を必要とする末端となるものと既存のネットワークとの間にOpenFlowスイッチを設置し、ここで全てのパケットを制御することで、あたかも独立したネットワークがあるかのように通信させます。このように、エッジオーバレイとは、末端に近いところで、いくつものネットワークを重ねて自由に定義するモデルを言います。また、この場所に存在するスイッチを一般に、エッジスイッチと呼びます。

[[edge_network_virtualization]]
image::edge_network_virtualization.png[caption="図17-1",title="エッジオーバーレイによるネットワーク仮想化"]

エッジスイッチの主な仕事は、物理ネットワークと仮想ネットワーク間でのパケットの相互書き換えです。

1. 仮想マシンから仮想ネットワークに送信したパケットは、エッジスイッチが物理ネットワークを通るように書き換え、宛先のサーバへ送出する
2. 宛先のサーバに届く直前のエッジスイッチで逆の書き換えを行う。つまり、物理ネットワークを通ってきたパケットを仮想ネットワーク内のパケットに見えるように書き換える

こうしたエッジスイッチによるパケットの書き換えはサーバからは見えません。OpenVNetの作り出した仮想ネットワークが、仮想マシンからは物理ネットワークであるように見えます。

[[edge_translation]]
image::edge_translation.png[caption="図17-2",title="エッジによるパケットの書き換え"]

OpenVNetのもう一つの大きな特長は、OpenFlow化されていない既存のネットワーク上で動作することです。たとえば <<sliceable_switch,第17章「ネットワークを仮想化する」>> で紹介したスライサブルスイッチには、ネットワークスイッチがすべてOpenFlowに対応しているという前提がありました。一方OpenVNetでは、この制御を物理サーバ上に起動したエッジスイッチだけで行います。こうすることで、既に構築されたネットワークの上で仮想ネットワークを実現できます。

// TODO: この説明は高宮が勝手に追加しましたが、合っていますか？
// あってます (山崎)

// TODO: 図が欲しい。物理ネットワーク(L2, L3, VPNでDB跨ぎ)に、仮想ネットワークをマッピングする図 = 基本的な考え方として理解できるもの

== エッジオーバーレイ仮想ネットワークの利点

OpenVNetのようなエッジオーバーレイ型は、次の2つの場面で特に威力を発揮します。

1. 既存データセンターの活用
2. ダウンサイジング

=== 既存データセンターの活用

最小の変更だけで既存データセンター上に仮想ネットワークサービスを構築できます。エッジオーバーレイによるネットワーク仮想化はほぼ物理サーバの追加だけでできます。このため、既存の物理ネットワークの敷設や再設定をできるだけ抑えながら、その上に新しく仮想ネットワークを構築して提供できるようになります。

=== ダウンサイジング

ネットワークの仮想化によりネットワークの収容効率を改善できます。たとえばサーバーの世界では、場所を取っていた大量の古い物理サーバーを仮想化し少数の物理サーバーに大量に詰め込み、さらにワークロードのばらつきを利用することで、収容効率が大幅に向上しました。仮想ネットワークでも、これと同じダウンサイジングが可能です。

近年のネットワーク帯域向上により「詰め込み」がますます現実的になってきました。たとえば10Gbpsの物理ネットワークには、単純計算すれば10Mbpsの仮想ネットワークを100個収容できます。さらに、それぞれのネットワークはいつも10Mbps使いきっているわけではありませんから、ばらつきを考慮し効率的に配置すれば、より多くを収容できます。これによって、古い大量の物理スイッチを仮想ネットワーク化することで一掃できます。

== 全体アーキテクチャ

OpenVNetのアーキテクチャは非常にシンプルです。vnmgr(Virtual Network Manager)が、ネットワーク全体の構造を保持するデータベースと、Web APIを提供します。データベースから、仮想ネットワークのあるべき設定を割り出したvnmgrは、分散するvna(Virtual Network Agent)に、担当するエッジスイッチに対しフローを設定するよう指示するのです。vnaは、接続されたスイッチ(Open vSwtich)に対し、OpenFlow仕様を含むフローの設定と、OpenFlow Controllerとして、DHCPなど反応すべきパケットに対する処理を任されています。vna内部で、OpenFlow Controllerの機能を実現するために、Tremaを使っています。

[[openvnet_architecture]]
image::openvnet_architecture.png[caption="図xx-y",title="OpenVNetの全体アーキテクチャ"]

vnctlは、OpenVNet全体を操作するためのコマンドラインインタフェースです。vnctlの引数に与えられたものを解釈して、vnmgrのWeb APIへ通信します。vnmgrは分散するvnaとの通信に、メッセージキューを利用することで、大規模なユースケースに対応する他、vnaが適切なタイミングでOpenFlow Switchと対話できるように設計されています。
現在、サポートしているソフトウェアセットは下記の通りです。

|===
| コンポーネント名称 | サポートしているソフトウェア名称 | URL

| OpenFlow Switch | Open vSwitch | http://openvswitch.org/
| Database | MySQL | http://www.mysql.com/
| Message Queue | ZeroMQ & Redis | http://zeromq.org/ http://redis.io/
|===

== OpenVNetの主な機能

OpenVNetが提供するたくさんの機能のうち、代表的なものは次の5つです。

1. 仮想ネットワークの作成
2. 仮想ネットワーク間のルーティング
3. セキュリティグループ
4. DHCPとDNS
5. 既存ネットワークと仮想ネットワークの接続

=== 仮想ネットワークの作成

仮想マシンのネットワークインタフェースが、あたかも同じスイッチに接続されたように見える機能です。例えば、ロードバランサー配下のWebサーバに対するスイッチ、Webサーバからデータベースサーバに対するスイッチなど、必要となるスイッチを任意に作成することができ、Webサーバやデータベースサーバと言った仮想マシンのネットワークインタフェースを、自由に接続することができます。

// TODO: 以下、それぞれの項目ごとに簡単な図がほしい

物理ネットワーク上にある物理スイッチに、同じIPアドレスを用いる仮想ネットワークを複数作っても問題はありません。エッジスイッチではそれら仮想ネットワークは全て適切に識別され、パケットの輻輳が起こらないように設計されています。

=== 仮想ネットワーク間のルーティング

作成した２つ以上の仮想ネットワークの間を自由にルーティングできます。これは、ルータを仮想的に配置するようなものです。

[[route_between_vnets]]
image::route_between_vnets.png[caption="図xx-y",title="仮想ネットワーク間のルーティング"]

ただし、Vyattaのような仮想ルータを実際に配置しているわけではなく、エッジスイッチのフローによって静的なルーティングを実現しています。仮想マシン間のパケットは余計なネットワーク経路を辿らず、エッジスイッチ間で最適な通信をします。もし動的なルーティングの機能が必要であれば、Vyattaを内蔵した仮想マシンを起動し、ネットワークインタフェースを複数持たせて、仮想ネットワークの間を動的にルーティングさせることもできます。

=== セキュリティグループ

エッジスイッチは各仮想マシンのトラフィック全ての関所でもあります。セキュリティグループは、この関所に、パケットの受け入れ許可ルールを指定し、仮想マシンのファイアウォールとして機能させるものです。

// TODO: 簡単な図がほしい
[[sequrity_groups]]
image::sequrity_groups.png[caption="図xx-y",title="セキュリティグループの役割"]

全ての仮想マシンをひとつずつ指定していく煩雑さを解消するため、論理名を付けたグルーピングと、グループ間の通信許可を指定することが出来るようになっています。特にグループ間の通信許可の場合は、グループに属する仮想マシンに変更があれば、相対するグループの設定にも動的に影響が及びます。OpenVNetは、このように分散したエッジスイッチの相互の影響を割り出し、常に相互の通信ルールが適切になるように制御します。

図では2つのセキュリティグループを挙げています。例えば、セキュリティグループBに対し、セキュリティグループAからのみアクセスの許可を指示できます。その時、セキュリティグループBに属する仮想マシンのエッジスイッチには、セキュリティグループAに属している仮想マシン1, 2, 3からの通信のみ許可するフローが入ります。インターネットからのアクセスのような、セキュリティグループA以外の通信はアクセスを許しません。通信させたいところを明示するため、仮想マシンのファイアウォールとなります。

=== DHCPとDNS

DHCPやDNSなどのサービスをエッジスイッチで処理できます。

// TODO: 簡単な図がほしい
[[dhcp]]
image::dhcp.png[caption="図xx-y",title="DHCPの通信例"]

本来は、ネットワーク上にDHCPサーバを設置し、そのサーバがDHCPのディスカバリ(IPアドレスの問い合わせパケット)に応答することになっています。しかし、わざわざDHCPサーバまで到達させずとも、OpenFlow Controllerで処理することもできます。エッジスイッチでパケットをマッチさせ、vnaの持つOpenFlow Controllerにエスカレーションし、DHCPのパケットを生成して、仮想マシンへ折り返してしまうことができます。仮想マシンに割り振られるIPアドレスが自明である場合に、この機能が利用できます。

=== 既存ネットワークと仮想ネットワークの接続

仮想ネットワークの世界の境界(VNetEdgeと呼びます)を外界と接続するための橋渡し方式を提供します。

// TODO: 簡単な図がほしい

仮想ネットワークは、最初はどこにも接続されていないスイッチのように振る舞い、閉じたネットワークとして作成されます。物理ネットワーク上にオーバレイされた、新しい仮想ネットワークですので、既存のネットワークからどのようにしてパケットを送受信しあうかも重要なポイントになります。VNetEdgeで受け取ったパケットを読み取り、仮想ネットワークへ流し込むルールを決めるトランスレーションと言う方法があります。トランスレーションは、パケットに記載されている情報を元にした条件を記述することで、条件にマッチしたパケットを指定された仮想ネットワークへと転送するものです。例えば、特定のTagged VLANのIDを持ったパケットを、任意の仮想ネットワークへ転送してみたり、特定のIPアドレスから送られてきたパケットを、任意の仮想ネットワークのIPアドレスへ転送しNATのようにしてみたりできます。

// ------------------------------------------------------------------

== 使ってみる

OpenVNetの利用はとても簡単です。まずは、CentOSが稼働する1台のマシンにOpenVNetの全てのサービスをインストールし、使い初めてみましょう。
マシンは、物理マシンでも仮想マシンでも構いません。要件は以下の2つだけです。

- CentOS 6.6以上が稼働するマシン
- インターネット接続

[[openvnet_installation_overview]]
image::openvnet_installation_overview.png[caption="図17-1",title="1台のマシンで動作するOpenVNet環境"]

=== インストールしてみる

OpenVNetのインストールと初期設定は、以下の手順で進んでいきます。

. OpenVNetのインストール
. Redis、MySQLのインストール
. エッジスイッチ設定
. 各種サービスの起動

それでは、この順序に沿ってOpenVNetをインストールしてみましょう。

==== OpenVNetのインストール

`openvnet.repo` をダウンロードし、 `/etc/yum/repos.d/` ディレクトリに配置します。

```
$ sudo curl -o /etc/yum.repos.d/openvnet.repo -R https://raw.githubusercontent.com/axsh/openvnet/master/deployment/yum_repositories/stable/openvnet.repo
```

次に、 `openvnet-third-party.repo` をダウンロードし、 `/etc/yum.repos.d/` ディレクトリに配置します。

```
$ sudo curl -o /etc/yum.repos.d/openvnet-third-party.repo -R https://raw.githubusercontent.com/axsh/openvnet/master/deployment/yum_repositories/stable/openvnet-third-party.repo
```

それぞれのリポジトリは、以下のパッケージを含んでいます。

* `openvnet.repo`
** `openvnet`
** `openvnet-common`
** `openvnet-vna`
** `openvnet-vnmgr`
** `openvnet-webapi`
** `openvnet-vnctl`
* `openvnet-third-party.repo`
** `openvnet-ruby`
** `openvswitch`

`openvnet` パッケージはメタパッケージで、 `openvnet-common` 、 `openvnet-vna` 、 `openvnet-vnmgr` 、 `openvnet-webapi` 、および `openvnet-vnctl` パッケージに依存しています。一度に全てをインストールするために便利なパッケージです。

なお、OpenVNetのインストールには `epel` が必要ですので、 `epel-release` パッケージをインストールしておきます。

```
$ sudo yum install -y epel-release
```

ここまでが完了したら、OpenVNetパッケージをインストールします。

```
$ sudo yum install -y openvnet
```


==== Redis、MySQLのインストール

RedisおよびMySQL serverパッケージをインストールします。RedisはOpenVNetのプロセス間通信に必要で、MySQLはネットワーク構成情報を保持する為に利用されます。

```
$ sudo yum install -y mysql-server redis
```

[NOTE]
RedisとMySQLのインストールについて
====
RedisとMySQLは両方必要とされていますが、OpenVNetは分散型のソフトウェアであるため、OpenVNetパッケージがこれらに依存する形にはなっていません。商用環境では、OpenVNetのプロセス群が動作するマシンとは異なるマシンにインストールされる形態を採用すると良いでしょう。
====

==== エッジスイッチ設定

`br0` という名前のエッジスイッチを作成します。後の疎通確認では、 `inst1` および `inst2` という2つの仮想マシンをこのエッジスイッチに接続します。 `br0` の設定ファイルとして、 `/etc/sysconfig/network-scripts/ifcfg-br0` を、以下の内容で作成します。

```
DEVICE=br0
DEVICETYPE=ovs
TYPE=OVSBridge
ONBOOT=yes
BOOTPROTO=static
HOTPLUG=no
OVS_EXTRA="
 set bridge     ${DEVICE} protocols=OpenFlow10,OpenFlow12,OpenFlow13 --
 set bridge     ${DEVICE} other_config:disable-in-band=true --
 set bridge     ${DEVICE} other-config:datapath-id=0000aaaaaaaaaaaa --
 set bridge     ${DEVICE} other-config:hwaddr=02:01:00:00:00:01 --
 set-fail-mode  ${DEVICE} standalone --
 set-controller ${DEVICE} tcp:127.0.0.1:6633
"
```

なお、この設定では `datapath-id` を `0000aaaaaaaaaaaa` という値に設定していますが、この値はOpenVNetがエッジスイッチを認識するための一意な識別子です。この値には16進数の値を設定できますが、後ほど利用する値ですので、憶えておいて下さい。

==== 各種サービスの起動

`openvswitch` サービスの起動と、エッジスイッチの起動を行います。

```
$ sudo service openvswitch start
$ sudo ifup br0
```

ネットワーク構成情報を保持するデータベースとしてインストールした、MySQL serverを起動します。

```
$ sudo service mysqld start
```

OpenVNetは、OpenVNet自身に内包されたRubyを利用しますので、環境変数PATHにそのパスを設定しておく必要があります。

```
$ PATH=/opt/axsh/openvnet/ruby/bin:${PATH}
```

Rubyにパスを通したら、データベースの作成を行います。

```
$ cd /opt/axsh/openvnet/vnet
$ bundle exec rake db:create
$ bundle exec rake db:init
```

先程述べたように、OpenVNetの各サービスはRedisで通信しますので、Redisを起動します。

```
$ service redis start
```

次に、OpenVNetのサービス群( `vnmgr` 、 `webapi` 、 `vna` )を起動します。これらを起動すると、 `/var/log/openvnet` ディレクトリにログが出力されます。もしうまく動作しない場合、このログの中に有用なエラーメッセージを見つけられる可能性があります。それでは、vnmgrとwebapiを起動してみましょう。

```
$ sudo initctl start vnet-vnmgr
$ sudo initctl start vnet-webapi
```

続いて、データベースのレコードを作成するのは、 `vnctl` ユーティリティを使用します。 `vnctl` は `openvnet-vnctl` パッケージに含まれる、WebAPIのクライアントです。先程、エッジスイッチの作成を行った際に設定した `datapath-id` の値を憶えているでしょうか？次のコマンドで、 `vna` がどの `datapath` を管理すればよいかをOpenVNetに教えます。

```
$ vnctl datapaths add --uuid dp-test1 --display-name test1 --dpid 0x0000aaaaaaaaaaaa --node-id vna
```

`vna` がどの `datapath` を管理すれば良いかの紐付けを行ったら、 `vna` を起動してみましょう。

```
$ sudo initctl start vnet-vna
```

`ovs-vsctl` コマンドで、 `vna` が正しく動作しているかを確認することができます。

```
$ ovs-vsctl show
```

ここで、 `is_connected: true` の文字列が見えていれば、 `vna` は正しく動作しています。もしこの文字列が見えない場合、数秒待ってから再施行してみて下さい。それでも見えない場合、 `/var/log/openvnet/vna.log` を確認し、何か問題が起こっていないかを確認して下さい。

```
fbe23184-7f14-46cb-857b-3abf6153a6d6
    Bridge "br0"
        Controller "tcp:127.0.0.1:6633"
            is_connected: true
```

ここまででOpenVNetのインストールと設定は完了し、動作を開始しましたが、まだOpenVNetの仮想ネットワークに接続する仮想マシンが作成されていません。そこで、次に仮想マシンとして2つの仮想マシン( `inst1` と `inst2` )を作成し、OpenVNetの仮想ネットワークに接続してみます。仮想マシンは、どのような仮想化技術のものでも動作します。今回は、軽量かつ仮想マシン内にも簡単に構築できるコンテナ技術を利用した、LXCと呼ばれるソフトウェアをインストールし、仮想マシンとして利用することにします。

```
$ sudo yum -y install lxc lxc-templates
```

`lxc` および `lxc-templates` パッケージのインストールが完了したら、コンテナ技術のうち、リソース制御を行う `cgroup` の利用準備を行います。

```
$ sudo mkdir /cgroup
$ echo "cgroup /cgroup cgroup defaults 0 0" >> /etc/fstab
$ sudo mount /cgroup
```

また、 `rsync` が必要になりますので、もしインストールされていない場合、以下のコマンドでrsyncをインストールして下さい。

```
$ sudo yum install -y rsync
```

LXCの動作の準備が出来ましたので、いよいよ仮想マシンの作成に入ります。

```
$ sudo lxc-create -t centos -n inst1
$ sudo lxc-create -t centos -n inst2
```

`lxc-create` を実行すると、それぞれの仮想マシンの `root` ユーザのパスワードがどこを見れば判るかが出力されます。このパスワードは後で仮想マシンにログインする際に利用しますので、憶えておいて下さい。次に、仮想マシンのネットワークインタフェースの設定を行います。 `/var/lib/lxc/inst1/config` ファイルを開き、内容を以下で置き換えて下さい。

```
lxc.network.type = veth
lxc.network.flags = up
lxc.network.veth.pair = inst1
lxc.network.hwaddr = 10:54:FF:00:00:01
lxc.rootfs = /var/lib/lxc/inst1/rootfs
lxc.include = /usr/share/lxc/config/centos.common.conf
lxc.arch = x86_64
lxc.utsname = inst1
lxc.autodev = 0
```

同様に、 `/var/lib/lxc/inst2/config` ファイルを開き、内容を以下で置き換えます。

```
lxc.network.type = veth
lxc.network.flags = up
lxc.network.veth.pair = inst2
lxc.network.hwaddr = 10:54:FF:00:00:02
lxc.rootfs = /var/lib/lxc/inst2/rootfs
lxc.include = /usr/share/lxc/config/centos.common.conf
lxc.arch = x86_64
lxc.utsname = inst2
lxc.autodev = 0
```

設定ファイルの内容を置き換えたら、仮想マシンを起動します。

```
$ sudo lxc-start -d -n inst1
$ sudo lxc-start -d -n inst2
```

仮想マシンが起動したら、その仮想マシンのネットワークインタフェースを先程設定したエッジスイッチに手動で接続します。これは、基本的にネットワークのケーブルを物理スイッチに挿入するのと同じです。

```
$ sudo ovs-vsctl add-port br0 inst1
$ sudo ovs-vsctl add-port br0 inst2
```

これで、OpenVNetのインストールと、OpenVNetの仮想ネットワークを体験する準備が整いました。ここまでの操作では、何もない物理ネットワークと繋がるエッジスイッチに仮想マシンが接続されているだけの状態です。

[[openvnet_connected]]
image::openvnet_connected.png[caption="図17-1",title="仮想マシンがエッジスイッチに接続された状態"]

次の節では、最も基本的な1つの仮想ネットワークの作成を試してみます。

=== CLIで操作してみる

仮想ネットワークの作成などの操作は、前節でも登場した `vnctl` で行うことが出来ます。まずは、1つの仮想ネットワークを作成してみましょう。

作成する仮想ネットワークのアドレスを `10.100.0.0/24` とし、 `inst1` のIPアドレスを `10.100.0.10`、`inst2` のIPアドレスを `10.100.0.11`とします。それでは、 `vnctl` コマンドを使用して仮想ネットワークを作成してみます。 `vnctl` コマンドで作成する対象は、 `networks` です。

```
$ vnctl networks add \
  --uuid nw-test1 \
  --display-name testnet1 \
  --ipv4-network 10.100.0.0 \
  --ipv4-prefix 24 \
  --network-mode virtual
```

この1つのコマンドだけで、仮想ネットワークが作成されました。

[[openvnet_cli_simplenetwork_1]]
image::openvnet_cli_simplenetwork_1.png[caption="図17-1",title="仮想ネットワークの作成"]

次に、どのIPアドレスを持つどのネットワークインタフェースが、その仮想ネットワークに所属しているのかを `vnctl` コマンドでOpenVNetに教えます。 操作する対象は、 `interfaces` です。まずは、 `inst1` の持つネットワークインタフェースを仮想ネットワークに設定します。

```
vnctl interfaces add \
  --uuid if-inst1 \
  --mode vif \
  --owner-datapath-uuid dp-test1 \
  --mac-address 10:54:ff:00:00:01 \
  --network-uuid nw-test1 \
  --ipv4-address 10.100.0.10 \
  --port-name inst1
```

同様に、 `inst2` の持つネットワークインタフェースを仮想ネットワークに設定します。

```
vnctl interfaces add \
  --uuid if-inst2 \
  --mode vif \
  --owner-datapath-uuid dp-test1 \
  --mac-address 10:54:ff:00:00:02 \
  --network-uuid nw-test1 \
  --ipv4-address 10.100.0.11 \
  --port-name inst2
```

この操作により、OpenVNetは `10.100.0.0/24` の仮想ネットワークを作成し、そこにそれぞれ `10.100.0.10` 、 `10.100.0.11` のIPアドレスを持つネットワークインタフェースが接続されていることを定義しました。

[[openvnet_cli_simplenetwork_2]]
image::openvnet_cli_simplenetwork_2.png[caption="図17-1",title="ネットワークインタフェースのIPアドレス定義を行った状態"]

=== 疎通確認をする

最後に、2つの仮想マシンが仮想ネットワークを通じて疎通ができることを確認します。まず `inst1` にログインし、IPアドレスを確認してみます。

```
$ lxc-console -n inst1
$ ip addr show
```

この操作時点ではまだ `inst1` の `eth0` にIPアドレスを付与していないため、IPアドレスが表示されませんが、これは正しい動作です。
先程作成した仮想ネットワークはDHCPサービスを有効にしていないため、IPアドレスは手動で付与する必要があります。

それでは、`inst1` の `eth0` にIPアドレスを付与します。付与するIPアドレスは、`vnctl` で `inst1` のネットワークインタフェースのIPアドレスとして設定した `10.100.0.10` です。

```
$ ip addr add 10.100.0.10/24 dev eth0
```

もう1つ端末を開き、 `inst2` に対し同じ操作を行います。ここで `inst2` の `eth0` に付与するIPアドレスは、 `10.100.0.11` です。

```
$ lxc-console -n inst2
$ ip addr add 10.100.0.11/24 dev eth0
```

これで2つの仮想マシンに仮想ネットワーク内のIPアドレスが付与されました。

[[openvnet_cli_simplenetwork_3]]
image::openvnet_cli_simplenetwork_3.png[caption="図17-1",title="ネットワークインタフェースにIPアドレスを付与"]

それでは、お互いに `ping` を実行してみます。まずは、 `inst2` から `inst1` に `ping` を実行します。

```
$ ping 10.100.0.10
```

うまく行った場合、pingは正しく動作し、疎通が確認できるはずです。もしうまく動作しない場合は、ここまでの手順で誤りがなかったかを確認してみて下さい。
疎通ができるようになったところで、注目すべき点として、従来のネットワークとOpenVNetの仮想ネットワークとの違いを1つ紹介します。

先程 `inst2` の `eth0` に設定したIPアドレスを、 `10.100.0.11/24` から `10.100.0.15/24` に変更してみましょう。

```
$ sudo ip addr del 10.100.0.11/24 dev eth0
$ sudo ip addr add 10.100.0.15/24 dev eth0
```

設定が終わったら、また `inst1` に対して `ping` を実行してみます。

```
$ ping 10.100.0.10
```

うまく動作したでしょうか。先程とは異なり、疎通ができなくなったことが確認できるはずです。これがもし従来のネットワークだった場合、 `10.100.0.0/24` の範囲内のIPアドレスに変更したとしても疎通できますが、OpenVNetはデータベースに従ってより厳格に制限を行うため、`inst2` のIPアドレスが `10.100.0.11` でない限り、通信を許可しません。

=== フローの変化を見る

OpenVNetはOpenFlowで仮想ネットワークをコントロールしていますが、フローエントリを `ovs-ofctl` でそのまま確認するのは大変です。
OpenVNetには `vna` と共にインストールされる `vnflows-monitor` というツールが付属しており、
フロー制御の節で解説したOpenVNetのフローテーブルの分類に基づいて、現在のエッジスイッチのフローエントリを読みやすく整形して表示してくれます。

`vnflows-monitor` を実行するには、OpenVNetが内包するRubyにパスが通っている必要があります。

```
$ PATH=/opt/axsh/openvnet/ruby/bin:${PATH}
```

それでは、 `vnflows-monitor` でフローエントリを表示してみましょう。

```
$ cd /opt/axsh/openvnet/vnet/bin/
$ ./vnflows-monitor
```

エッジスイッチが正しく動作していて、フローエントリが存在する場合、例として以下のような内容が表示されます。

```
(0): TABLE_CLASSIFIER
  0-00        0       0 => SWITCH(0x0)               actions=write_metadata:REMOTE(0x0),goto_table:TABLE_TUNNEL_PORTS(3)
  0-01        0       0 => SWITCH(0x0)              tun_id=0 actions=drop
  0-02       28       0 => PORT(0x1)                in_port=1 actions=write_metadata:TYPE_INTERFACE|LOCAL(0x1),goto_table:TABLE_INTERFACE_EGRESS_CLASSIFIER(15)
  0-02       22       0 => PORT(0x2)                in_port=2 actions=write_metadata:TYPE_INTERFACE|LOCAL(0x5),goto_table:TABLE_INTERFACE_EGRESS_CLASSIFIER(15)
  0-02        0       0 => SWITCH(0x0)              in_port=CONTROLLER actions=write_metadata:LOCAL|NO_CONTROLLER(0x0),goto_table:TABLE_CONTROLLER_PORT(7)
  0-02        0       0 => PORT(0x7ffffffe)         in_port=LOCAL actions=write_metadata:LOCAL(0x0),goto_table:TABLE_LOCAL_PORT(6)
(3): TABLE_TUNNEL_PORTS
  3-00        0       0 => SWITCH(0x0)               actions=drop
(4): TABLE_TUNNEL_NETWORK_IDS
  4-00        0       0 => SWITCH(0x0)               actions=drop
  4-30        0       0 => ROUTE_LINK(0x1)          tun_id=0x10000001,dl_dst=02:00:10:00:00:01 actions=write_metadata:TYPE_ROUTE_LINK(0x1),goto_table:TABLE_ROUTER_CLASSIFIER(33)
  4-30        0       0 => NETWORK(0x1)             tun_id=0x80000001 actions=write_metadata:TYPE_NETWORK(0x1),goto_table:TABLE_NETWORK_SRC_CLASSIFIER(20)
  4-30        0       0 => NETWORK(0x2)             tun_id=0x80000002 actions=write_metadata:TYPE_NETWORK(0x2),goto_table:TABLE_NETWORK_SRC_CLASSIFIER(20)
(6): TABLE_LOCAL_PORT
  6-00        0       0 => SWITCH(0x0)               actions=drop
...
```

このように、フローエントリが (0): TABLE_CLASSIFIER といった形で、OpenVNetのフローテーブルの分類でグループ化されて表示されます。
また、それぞれのフローテーブルの下に表示される行の意味は、左から順に、以下のようになっています。

. フローエントリの優先度に従ったフローテーブルのインデックス (0-00、0-01など)
. そのフローエントリにマッチしたパケット数 (0、28、22など)
. フローの `cookie` (0 => SWITCH(0x0)など)
. フローの `match` (tun_id=0、in_port=1など)
. フローの `action` (actions=dropなど)

== OpenVNetを応用した実用例

最後にOpenVNetを応用した事例として、TIS株式会社が実装したOpenVNetに関する二つの実用例について紹介いたします。一つは複数サーバ上のDockerコンテナを仮想ネットワークで接続する事例、もう一つは複数のクラウド間を仮想ネットワークで連結する事例です。

=== 複数サーバ上のDockerコンテナを仮想ネットワークで接続する

そもそもDocker footnote:[Dockerの詳細は、Dockerの公式ドキュメント(https://docs.docker.com/)を参照ください] とは、dotCloud社（現Docker社）が自社のパブリックPaaSを実現するために開発した技術を公開したものです。アプリケーションの実行環境を容易に素早く、かつ他の影響を受けないようにして立ち上げるために、Dockerは様々なLinuxの技術を用いて「他から隔離された環境（＝コンテナ） 」を作り出します。

Dockerは様々なリソースを隔離しますが、ネットワークもその隔離すべきリソースの一つです。そのためDockerは、ネットワークネームスペースや仮想ネットワークインタフェース等の技術を用いて、Linuxサーバ上に他から隔離された内部ネットワークを構成します。ただしそのままではサーバの外部と通信ができませんので、Dockerは通常、iptablesの機能を用いて外部ネットワークと連携できるようにします。

[[docker_network]]
image::docker_network.png[caption="図17-1",title="Dockerのネットワーク"]

単独のサーバ内でDockerを利用するだけならこの方式で良いのですが、複数のサーバでDockerを動作させたい場合には問題が生じます。Dockerコンテナが所属するネットワークはサーバ内に閉じていますので、異なるサーバで動作しているDockerコンテナ同士が、そのDockerコンテナに付与されたIPアドレスで通信することができないのです。

この問題を解決するために様々なDockerネットワーキングツールが公開されていますし、2015年10月にリリースされたDocker v1.9からは、Docker自身が複数サーバを跨った仮想ネットワークを構成できるようになりました。

しかし現時点で、DHCPやDNS相当の機能を持ったツールはありますが、セキュリティグループに相当する機能が実装されたツールはないようです。そこでOpenVNetを用います。OpenVNetを用いて敷設した仮想ネットワークにDockerコンテナを接続すれば、サーバを跨ったDockerコンテナ間でもシームレスに通信できるだけでなく、Dockerのネットワークにセキュリティグループの機を付与できるようになります。

例えば同一の物理ネットワークに接続したサーバ2台と、普通のルータを挟んで別の物理ネットワークに接続したサーバ1台の合計3つのサーバがあるとします。それらのサーバ上でDockerコンテナを動作させ、それらをOpenVNetを用いて敷設した仮想ネットワークに接続することを考えてみましょう。

まず最初に、各サーバ上にエッジスイッチを立ち上げます。各エッジスイッチのDatapath IDが重複しないように注意してください。

次に各サーバ上でDockerコンテナを立ち上げ、Dockerコンテナを各サーバ内部に立ち上げたエッジスイッチに接続します。この際、後からOpenVNetに設定できるように、Dockerコンテナに与えた仮想ネットワークインタフェースのMACアドレスとIPアドレスをメモしておきましょう。

さらに以下の手順でOpenVNetを設定します。

* 各エッジスイッチのDatapath IDをOpenVNetに設定する
* 各サーバが所属する物理ネットワークの情報をOpenVNetに設定する
* OpenVNetが敷設する仮想ネットワークを定義する
* 各サーバの物理ネットワークインタフェースの情報をOpenVNetに設定する
* 立ち上げたDockerコンテナの仮想ネットワークインタフェースの情報をOpenVNetに設定する
* OpenVNetが制御するセキュリティグループを定義する
* 各仮想ネットワークインタフェースに望みのセキュリティグループを割り当てる
* OpenVNet上に仮想ルータを構成して、物理ネットワークと仮想ネットワーク間のルーティングを定義する

最後に各サーバとDockerコンテナにスタティックルートを設定すれば、OpenVNetを用いたDockerネットワーキングが完成します。

各サーバ上のDockerコンテナは、OpenVNetが敷設した同じ仮想ネットワークに接続していますので、異なるサーバのDockerコンテナであってもそのIPアドレスを用いて通信できます。またセキュリティグループの設定に従い、到達すべきでないパケットはOpenVNetがDROPするため、個々のDockerコンテナにパケットフィルタルールを定義する必要が無くなります。

image::docker_openvnet_1.png[caption="図17-2",title="OpenVNetを用いたDockerネットワーキング"]

なお、ここで説明した手順を実際に実行し動作させるツールキットを、walfisch footnote:[https://github.com/tech-sketch/walfisch] というフリーソフトウェアとして公開しています。実際に実行したコマンドが標準出力に表示されますので、OpenVNetを用いたDockerネットワーキングに興味がある方は一度動作させてみると良いでしょう。

=== 複数のクラウド間を仮想ネットワークで連結する

先ほどは複数サーバ上のDockerコンテナ間を連結するというミクロな視点で仮想ネットワークを利用しました。しかし仮想ネットワークはそのようなミクロな視点だけでなく、複数のクラウド間を連結するというマクロな視点でも重要な役割を果たします。

パブリックもプライベートも、現在様々なクラウドが利用されていますが、提供されるネットワーク機能やその利用方法はクラウドごとに大きく異なります。そのため複数のクラウド間を連結したい場合、それぞれのクラウドのネットワーク機能を強く意識したネットワーク設計を行う必要があります。

一方OpenVNetは、フローによってOpenVNetの仮想ネットワークと外部のネットワークの間をシームレスに接続するVNetEdge機能を持っています。そのためOpenVNetを利用することで、クラウドが提供するネットワーク機能に依存せず、複数のパブリックあるいはプライベートクラウドをシームレスに連携することが可能となります。

例えば、プライベートクラウドとしてWakame-vdc、パブリッククラウドとしてAmazon Web Servicesのネットワークを連結する構成例を考えてみます。

[[narukozaka_tools]]
image::narukozaka_tools.png[caption="図17-4",title="プライベートIaaSとパブリックIaaSの連結構成"]

この構成例では、仮想ネットワークIDとVLAN IDの変換規則をOpenVNetに登録しておくことで、wakame-vdcの仮想ネットワークと、Amazon Web ServicesのVirtual Private Cloudで構築されたネットワークの間を流れるパケットがVNetEdgeのOpen vSwitchを通過する際に、この２つのネットワークが同一のネットワークであるかのようにパケット転送を制御します。

このツールキットはフリーソフトウェアとして公開しており footnote:[https://github.com/cloudconductor-incubator/narukozaka-tools]、この他にも多くの機能を持ちます。

* IaaSのインスタンスイメージの作成と起動
* IaaSのインスタンスにインストールするミドルウェアの自動設定
* IaaSのネットワーク上に、VNetEdgeをスイッチとしたスター型のネットワークトポロジを構築する機能
* wakame-vdcとパブリックIaaSの間を自動的に連結する機能

またセキュリティの案件に応じ、wakame-vdc側のインスタンスとAmazon Web Services側のVNetEdge間の通信を暗号化するといった、柔軟な対応も可能です。

== まとめ

* OpenVNetはオープンソースライセンスLGPL3に基づくフリーソフトウェアであり、バザール式のオープンな開発コミュニティを持っている
* エッジオーバレイ仮想ネットワークを実現できるため、物理ネットワークへの影響がほとんど無い
* オンプレミス環境以外にも、AWSに代表されるパブリッククラウドでも利用ができる
* 仮想マシンだけでなく、Dockerに代表されるコンテナが主体の基盤とも組み合わせて利用できる

最後に、OpenVNetは、Trema同様に、常時開発にご協力いただける方々を募集しております。腕に覚えのある方は、ぜひ下記の情報をご参照の上、奮ってご参加いただければ幸いです。

【URL】 http://openvnet.org/
[[wesite_openvnet]]
image::QR_to_openvnet.gif[caption="図xx-y",title="OpenVNetのサイトへのリンク"]
