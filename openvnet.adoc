= OpenVNetで本格的な仮想ネットワーク
:imagesdir: images/openvnet

// TODO ほかの章と同じく、リード文を2,3行ほど追加する

== OpenVNetとは

OpenVNetはOpenFlowで仮想ネットワークを構築するためのOSSです。Tremaを使ってあらゆるパケットの挙動を自由に制御することで、既存のネットワーク上にあたかもユーザ専用のネットワークがあるかのような環境を作り出すことができます。開発はWSF(Wakame Software Foundation)が中心となっており、筆者の一人である山崎の所属する株式会社あくしゅの開発者がメインコミッターを務めています。

=== 仮想ネットワークを構築するエッジオーバレイ型のOSS

OpenVNetによるネットワーク仮想化の特長は「エッジオーバーレイ型」である点です。仮想マシンのような通信を必要とする末端となるものと既存のネットワークとの間にOpenFlowスイッチを設置し、ここで全てのパケットを制御することで、あたかも独立したネットワークがあるかのように通信させます。このように、エッジオーバレイとは、末端に近いところで、いくつものネットワークを重ねて自由に定義するモデルを言います。また、この場所に存在するスイッチを一般に、エッジスイッチと呼びます。

[[edge_network_virtualization]]
image::edge_network_virtualization.png[caption="図17-1",title="エッジオーバーレイによるネットワーク仮想化"]

エッジスイッチの主な仕事は、物理ネットワークと仮想ネットワーク間でのパケットの相互書き換えです。

1. 仮想マシンから仮想ネットワークに送信したパケットは、エッジスイッチが物理ネットワークを通るように書き換え、宛先のサーバへ送出する
2. 宛先のサーバに届く直前のエッジスイッチで逆の書き換えを行う。つまり、物理ネットワークを通ってきたパケットを仮想ネットワーク内のパケットに見えるように書き換える

こうしたエッジスイッチによるパケットの書き換えはサーバからは見えません。OpenVNetの作り出した仮想ネットワークが、仮想マシンからは物理ネットワークであるように見えます。

[[edge_translation]]
image::edge_translation.png[caption="図17-2",title="エッジによるパケットの書き換え"]

OpenVNetのもう一つの大きな特長は、OpenFlow化されていない既存のネットワーク上で動作することです。たとえば <<sliceable_switch,第17章「ネットワークを仮想化する」>> で紹介したスライサブルスイッチには、ネットワークスイッチがすべてOpenFlowに対応しているという前提がありました。一方OpenVNetでは、この制御を物理サーバ上に起動したエッジスイッチだけで行います。こうすることで、既に構築されたネットワークの上で仮想ネットワークを実現できます。

// TODO: この説明は高宮が勝手に追加しましたが、合っていますか？

// TODO: 図が欲しい。物理ネットワーク(L2, L3, VPNでDB跨ぎ)に、仮想ネットワークをマッピングする図 = 基本的な考え方として理解できるもの

== エッジオーバーレイ仮想ネットワークの利点

OpenVNetのようなエッジオーバーレイ型は、次の2つの場面で特に威力を発揮します。

1. 既存データセンターの活用
2. ダウンサイジング

=== 既存データセンターの活用

最小の変更だけで既存データセンター上に仮想ネットワークサービスを構築できます。エッジオーバーレイによるネットワーク仮想化はほぼ物理サーバの追加だけでできます。このため、既存の物理ネットワークの敷設や再設定をできるだけ抑えながら、その上に新しく仮想ネットワークを構築して提供できるようになります。

=== ダウンサイジング

ネットワークの仮想化によりネットワークの収容効率を改善できます。たとえばサーバーの世界では、場所を取っていた大量の古い物理サーバーを仮想化し少数の物理サーバーに大量に詰め込み、さらにワークロードのばらつきを利用することで、収容効率が大幅に向上しました。仮想ネットワークでも、これと同じダウンサイジングが可能です。

近年のネットワーク帯域向上により「詰め込み」がますます現実的になってきました。たとえば10Gbpsの物理ネットワークには、単純計算すれば10Mbpsの仮想ネットワークを100個収容できます。さらに、それぞれのネットワークはいつも10Mbps使いきっているわけではありませんから、ばらつきを考慮し効率的に配置すれば、より多くを収容できます。これによって、古い大量の物理スイッチを仮想ネットワーク化することで一掃できます。

== OpenVNetの主な機能

OpenVNetが提供するたくさんの機能のうち、代表的なものは次の5つです。

1. 仮想ネットワークの作成
2. 仮想ネットワーク間のルーティング
3. セキュリティグループ
4. DHCPとDNS
5. 既存ネットワークと仮想ネットワークの接続

=== 仮想ネットワークの作成

仮想マシンのネットワークインタフェースが、あたかも同じスイッチに接続されたように見える機能です。例えば、ロードバランサー配下のWebサーバに対するスイッチ、Webサーバからデータベースサーバに対するスイッチなど、必要となるスイッチを任意に作成することができ、Webサーバやデータベースサーバと言った仮想マシンのネットワークインタフェースを、自由に接続することができます。

// TODO: 以下、それぞれの項目ごとに簡単な図がほしい

物理ネットワーク上にある物理スイッチに、同じIPアドレスを用いる仮想ネットワークを複数作っても問題はありません。エッジスイッチではそれら仮想ネットワークは全て適切に識別され、パケットの輻輳が起こらないように設計されています。

=== 仮想ネットワーク間のルーティング

作成した２つ以上の仮想ネットワークの間を自由にルーティングできます。これは、ルータを仮想的に配置するようなものです。

// TODO: 簡単な図がほしい

ただし、Vyattaのような仮想ルータを実際に配置しているわけではなく、エッジスイッチのフローによって静的なルーティングを実現しています。仮想マシン間のパケットは余計なネットワーク経路を辿らず、エッジスイッチ間で最適な通信をします。もし動的なルーティングの機能が必要であれば、Vyattaを内蔵した仮想マシンを起動し、ネットワークインタフェースを複数持たせて、仮想ネットワークの間を動的にルーティングさせることもできます。

=== セキュリティグループ

エッジスイッチは各仮想マシンのトラフィック全ての関所でもあります。セキュリティグループは、この関所に、パケットの受け入れ許可ルールを指定し、仮想マシンのファイアウォールとして機能させるものです。

// TODO: 簡単な図がほしい

全ての仮想マシンをひとつずつ指定していく煩雑さを解消するため、論理名を付けたグルーピングと、グループ間の通信許可を指定することが出来るようになっています。特にグループ間の通信許可の場合は、グループに属する仮想マシンに変更があれば、相対するグループの設定にも動的に影響が及びます。OpenVNetは、このように分散したエッジスイッチの相互の影響を割り出し、常に相互の通信ルールが適切になるように制御します。

=== DHCPとDNS

DHCPやDNSなどのサービスをエッジスイッチで処理できます。

// TODO: 簡単な図がほしい

本来は、ネットワーク上にDHCPサーバを設置し、そのサーバがDHCPのディスカバリ(IPアドレスの問い合わせパケット)に応答することになっています。しかし、わざわざDHCPサーバまで到達させずとも、応答すべきパケットが自明である場合は、エッジスイッチでパケットを生成して、仮想マシンへ折り返してしまうことができます。ノードに割り振られるIPアドレスが自明である場合に、この機能が利用できます。

=== 既存ネットワークと仮想ネットワークの接続

仮想ネットワークの世界の境界(VNetEdgeと呼びます)を外界と接続するための橋渡し方式を提供します。

// TODO: 簡単な図がほしい

仮想ネットワークは、最初はどこにも接続されていないスイッチのように振る舞い、閉じたネットワークとして作成されます。物理ネットワーク上にオーバレイされた、新しい仮想ネットワークですので、既存のネットワークからどのようにしてパケットを送受信しあうかも重要なポイントになります。VNetEdgeで受け取ったパケットを読み取り、仮想ネットワークへ流し込むルールを決めるトランスレーションと言う方法があります。トランスレーションは、パケットに記載されている情報を元にした条件を記述することで、条件にマッチしたパケットを指定された仮想ネットワークへと転送するものです。例えば、特定のTagged VLANのIDを持ったパケットを、任意の仮想ネットワークへ転送してみたり、特定のIPアドレスから送られてきたパケットを、任意の仮想ネットワークのIPアドレスへ転送しNATのようにしてみたりできます。

== アーキテクチャと動作

OpenVNetのアーキテクチャは非常にシンプルです。vnmgr(Virtual Network Manager)が、ネットワーク全体の構造を保持するデータベースと、Web APIを提供します。データベースから、仮想ネットワークのあるべき設定を割り出したvnmgrは、分散するvna(Virtual Network Agent)に、担当するエッジスイッチに対しフローを設定するよう指示するのです。vnaは、接続されたスイッチ(Open vSwtich)に対し、OpenFlow仕様を含むフローの設定と、OpenFlow Controllerとして、DHCPなど反応すべきパケットに対する処理を任されています。vna内部で、OpenFlow Controllerの機能を実現するために、Tremaを使っています。

== 使ってみる

OpenVNetの利用はとても簡単です。まずは、CentOSが稼働する1台のマシンにOpenVNetの全てのサービスをインストールし、使い初めてみましょう。
マシンは、物理マシンでも仮想マシンでも構いません。要件は以下の2つだけです。

- CentOS 6.6以上が稼働するマシン
- インターネット接続

[[openvnet_installation_overview]]
image::openvnet_installation_overview.png[caption="図17-1",title="1台のマシンで動作するOpenVNet環境"]

=== インストールしてみる

OpenVNetのインストールと初期設定は、以下の手順で進んでいきます。

. OpenVNetのインストール
. Redis、MySQLのインストール
. エッジスイッチ設定
. 各種サービスの起動

それでは、この順序に沿ってOpenVNetをインストールしてみましょう。

==== OpenVNetのインストール

`openvnet.repo` をダウンロードし、 `/etc/yum/repos.d/` ディレクトリに配置します。

```
$ sudo curl -o /etc/yum.repos.d/openvnet.repo -R https://raw.githubusercontent.com/axsh/openvnet/master/deployment/yum_repositories/stable/openvnet.repo
```

次に、 `openvnet-third-party.repo` をダウンロードし、 `/etc/yum.repos.d/` ディレクトリに配置します。

```
$ sudo curl -o /etc/yum.repos.d/openvnet-third-party.repo -R https://raw.githubusercontent.com/axsh/openvnet/master/deployment/yum_repositories/stable/openvnet-third-party.repo
```

それぞれのリポジトリは、以下のパッケージを含んでいます。

* `openvnet.repo`
** `openvnet`
** `openvnet-common`
** `openvnet-vna`
** `openvnet-vnmgr`
** `openvnet-webapi`
** `openvnet-vnctl`
* `openvnet-third-party.repo`
** `openvnet-ruby`
** `openvswitch`

`openvnet` パッケージはメタパッケージで、 `openvnet-common` 、 `openvnet-vna` 、 `openvnet-vnmgr` 、 `openvnet-webapi` 、および `openvnet-vnctl` パッケージに依存しています。一度に全てをインストールするために便利なパッケージです。

なお、OpenVNetのインストールには `epel` が必要ですので、 `epel-release` パッケージをインストールしておきます。

```
$ sudo yum install -y epel-release
```

ここまでが完了したら、OpenVNetパッケージをインストールします。

```
$ sudo yum install -y openvnet
```


==== Redis、MySQLのインストール

RedisおよびMySQL serverパッケージをインストールします。RedisはOpenVNetのプロセス間通信に必要で、MySQLはネットワーク構成情報を保持する為に利用されます。これらは両方必要とされていますが、OpenVNetは分散型のソフトウェアであるため、OpenVNetパッケージがこれらに依存する形にはなっていません。商用環境では、OpenVNetのプロセス群が動作するマシンとは異なるマシンにインストールされる形態を採用すると良いでしょう。

```
$ sudo yum install -y mysql-server redis
```

==== エッジスイッチ設定

`br0` という名前のエッジスイッチを作成します。後の疎通確認では、 `inst1` および `inst2` という2つの仮想マシンをこのエッジスイッチに接続します。 `br0` の設定ファイルとして、 `/etc/sysconfig/network-scripts/ifcfg-br0` を、以下の内容で作成します。

```
DEVICE=br0
DEVICETYPE=ovs
TYPE=OVSBridge
ONBOOT=yes
BOOTPROTO=static
HOTPLUG=no
OVS_EXTRA="
 set bridge     ${DEVICE} protocols=OpenFlow10,OpenFlow12,OpenFlow13 --
 set bridge     ${DEVICE} other_config:disable-in-band=true --
 set bridge     ${DEVICE} other-config:datapath-id=0000aaaaaaaaaaaa --
 set bridge     ${DEVICE} other-config:hwaddr=02:01:00:00:00:01 --
 set-fail-mode  ${DEVICE} standalone --
 set-controller ${DEVICE} tcp:127.0.0.1:6633
"
```

なお、この設定では `datapath-id` を `0000aaaaaaaaaaaa` という値に設定していますが、この値はOpenVNetがエッジスイッチを認識するための一意な識別子です。この値には16進数の値を設定できますが、後ほど利用する値ですので、憶えておいて下さい。

==== 各種サービスの起動

`openvswitch` サービスの起動と、エッジスイッチの起動を行います。

```
$ sudo service openvswitch start
$ sudo ifup br0
```

ネットワーク構成情報を保持するデータベースとしてインストールした、MySQL serverを起動します。

```
$ sudo service mysqld start
```

OpenVNetは、OpenVNet自身に内包されたRubyを利用しますので、環境変数PATHにそのパスを設定しておく必要があります。

```
$ PATH=/opt/axsh/openvnet/ruby/bin:${PATH}
```

Rubyにパスを通したら、データベースの作成を行います。

```
$ cd /opt/axsh/openvnet/vnet
$ bundle exec rake db:create
$ bundle exec rake db:init
```

先程述べたように、OpenVNetの各サービスはRedisで通信しますので、Redisを起動します。

```
$ service redis start
```

次に、OpenVNetのサービス群( `vnmgr` 、 `webapi` 、 `vna` )を起動します。これらを起動すると、 `/var/log/openvnet` ディレクトリにログが出力されます。もしうまく動作しない場合、このログの中に有用なエラーメッセージを見つけられる可能性があります。それでは、vnmgrとwebapiを起動してみましょう。

```
$ sudo initctl start vnet-vnmgr
$ sudo initctl start vnet-webapi
```

続いて、データベースのレコードを作成するのは、 `vnctl` ユーティリティを使用します。 `vnctl` は `openvnet-vnctl` パッケージに含まれる、WebAPIのクライアントです。先程、エッジスイッチの作成を行った際に設定した `datapath-id` の値を憶えているでしょうか？次のコマンドで、 `vna` がどの `datapath` を管理すればよいかをOpenVNetに教えます。

```
$ vnctl datapaths add --uuid dp-test1 --display-name test1 --dpid 0x0000aaaaaaaaaaaa --node-id vna
```

`vna` がどの `datapath` を管理すれば良いかの紐付けを行ったら、 `vna` を起動してみましょう。

```
$ sudo initctl start vnet-vna
```

`ovs-vsctl` コマンドで、 `vna` が正しく動作しているかを確認することができます。

```
$ ovs-vsctl show
```

ここで、 `is_connected: true` の文字列が見えていれば、 `vna` は正しく動作しています。もしこの文字列が見えない場合、数秒待ってから再施行してみて下さい。それでも見えない場合、 `/var/log/openvnet/vna.log` を確認し、何か問題が起こっていないかを確認して下さい。

```
fbe23184-7f14-46cb-857b-3abf6153a6d6
    Bridge "br0"
        Controller "tcp:127.0.0.1:6633"
            is_connected: true
```

ここまででOpenVNetのインストールと設定は完了し、動作を開始しましたが、まだOpenVNetの仮想ネットワークに接続する仮想マシンが作成されていません。そこで、次に仮想マシンとして2つのLXCコンテナ( `inst1` と `inst2` )を作成し、OpenVNetの仮想ネットワークに接続してみます。どのような仮想化技術でも動作はしますが、今回は、軽量かつ仮想マシン内にも簡単に構築できるLXCをインストールし、利用することにします。

```
$ sudo yum -y install lxc lxc-templates
```

`lxc` および `lxc-templates` パッケージのインストールが完了したら、コンテナのリソース制御を行う `cgroup` の利用準備を行います。

```
$ sudo mkdir /cgroup
$ echo "cgroup /cgroup cgroup defaults 0 0" >> /etc/fstab
$ sudo mount /cgroup
```

また、 `rsync` が必要になりますので、もしインストールされていない場合、以下のコマンドでrsyncをインストールして下さい。

```
$ sudo yum install -y rsync
```

LXCの動作の準備が出来ましたので、いよいよ仮想マシンの作成に入ります。

```
$ sudo lxc-create -t centos -n inst1
$ sudo lxc-create -t centos -n inst2
```

`lxc-create` を実行すると、それぞれの仮想マシンの `root` ユーザのパスワードがどこを見れば判るかが出力されます。このパスワードは後で仮想マシンにログインする際に利用しますので、憶えておいて下さい。次に、仮想マシンのネットワークインタフェースの設定を行います。 `/var/lib/lxc/inst1/config` ファイルを開き、内容を以下で置き換えて下さい。

```
lxc.network.type = veth
lxc.network.flags = up
lxc.network.veth.pair = inst1
lxc.network.hwaddr = 10:54:FF:00:00:01
lxc.rootfs = /var/lib/lxc/inst1/rootfs
lxc.include = /usr/share/lxc/config/centos.common.conf
lxc.arch = x86_64
lxc.utsname = inst1
lxc.autodev = 0
```

同様に、 `/var/lib/lxc/inst2/config` ファイルを開き、内容を以下で置き換えます。

```
lxc.network.type = veth
lxc.network.flags = up
lxc.network.veth.pair = inst2
lxc.network.hwaddr = 10:54:FF:00:00:02
lxc.rootfs = /var/lib/lxc/inst2/rootfs
lxc.include = /usr/share/lxc/config/centos.common.conf
lxc.arch = x86_64
lxc.utsname = inst2
lxc.autodev = 0
```

設定ファイルの内容を置き換えたら、仮想マシンを起動します。

```
$ sudo lxc-start -d -n inst1
$ sudo lxc-start -d -n inst2
```

仮想マシンが起動したら、先述したとおり、起動したコンテナのネットワークインタフェースを先程設定したエッジスイッチに手動で接続します。これは、基本的にネットワークのケーブルを物理スイッチに挿入するのと同じです。

```
$ sudo ovs-vsctl add-port br0 inst1
$ sudo ovs-vsctl add-port br0 inst2
```

これで、OpenVNetのインストールと、OpenVNetの仮想ネットワークを体験する準備が整いました。次の節では、最も基本的な1つの仮想ネットワークセグメントの作成を試してみます。

=== CLIで操作してみる

仮想ネットワークの作成などの操作は、前節でも登場した `vnctl` で行うことが出来ます。まずは、1つの仮想ネットワークセグメントを作成してみましょう。

[[openvnet_cli_simplenetwork]]
image::openvnet_cli_simplenetwork.png[caption="図17-1",title="最も基本的な1つの仮想ネットワークセグメント"]

作成する仮想ネットワークのアドレスを `10.100.0.0/24.` とし、 inst1` のIPアドレスを `10.100.0.10`、`inst2` のIPアドレスを `10.100.0.11`とします。それでは、 `vnctl` コマンドを使用して仮想ネットワークを作成してみます。 `vnctl` コマンドで作成する対象は、 `networks` です。

```
$ vnctl networks add \
  --uuid nw-test1 \
  --display-name testnet1 \
  --ipv4-network 10.100.0.0 \
  --ipv4-prefix 24 \
  --network-mode virtual
```

この1つのコマンドだけで、仮想ネットワークが作成されました。次に、どのIPアドレスを持つどのネットワークインタフェースが、その仮想ネットワークに所属しているのかを `vnctl` コマンドでOpenVNetに教えます。 操作する対象は、 `interfaces` です。まずは、 `inst1` の持つネットワークインタフェースを仮想ネットワークに設定します。

```
vnctl interfaces add \
  --uuid if-inst1 \
  --mode vif \
  --owner-datapath-uuid dp-test1 \
  --mac-address 10:54:ff:00:00:01 \
  --network-uuid nw-test1 \
  --ipv4-address 10.100.0.10 \
  --port-name inst1
```

同様に、 `inst2` の持つネットワークインタフェースを仮想ネットワークに設定します。

```
vnctl interfaces add \
  --uuid if-inst2 \
  --mode vif \
  --owner-datapath-uuid dp-test1 \
  --mac-address 10:54:ff:00:00:02 \
  --network-uuid nw-test1 \
  --ipv4-address 10.100.0.11 \
  --port-name inst2
```

この操作により、OpenVNetは `10.100.0.0/24` の仮想ネットワークを作成し、そこにそれぞれ `10.100.0.10` 、 `10.100.0.11` のIPアドレスを持つネットワークインタフェースが接続されていることを定義しました。

=== 疎通確認をする

最後に、2つの仮想マシンが仮想ネットワークを通じて疎通ができることを確認します。まず `inst1` にログインし、IPアドレスを確認してみます。

```
$ lxc-console -n inst1
$ ip a
```

この操作時点ではまだ `inst1` の `eth0` にIPアドレスを付与していないため、IPアドレスが表示されませんが、これは正しい動作です。
先程作成した仮想ネットワークはDHCPサービスを有効にしていないため、IPアドレスは手動で付与する必要があります。

それでは、`inst1` の `eth0` にIPアドレスを付与します。付与するIPアドレスは、`vnctl` で `inst1` のネットワークインタフェースのIPアドレスとして設定した `10.100.0.10` です。

```
$ ip addr add 10.100.0.10/24 dev eth0
```

もう1つ端末を開き、 `inst2` に対し同じ操作を行います。ここで `inst2` の `eth0` に付与するIPアドレスは、 `10.100.0.11` です。

```
$ lxc-console -n inst2
$ ip addr add 10.100.0.11/24 dev eth0
```

これで2つの仮想マシンに仮想ネットワーク内のIPアドレスが付与されたので、お互いに `ping` を実行してみます。まずは、 `inst2` から `inst1` に `ping` を実行します。

```
$ ping 10.100.0.10
```

うまく行った場合、pingは正しく動作し、疎通が確認できるはずです。もしうまく動作しない場合は、ここまでの手順で誤りがなかったかを確認してみて下さい。
疎通ができるようになったところで、注目すべき点として、従来のネットワークとOpenVNetの仮想ネットワークとの違いを1つ紹介します。

先程 `inst2` の `eth0` に設定したIPアドレスを、 `10.100.0.11/24` から `10.100.0.15/24` に変更してみましょう。

```
$ sudo ip addr del 10.100.0.11/24 dev eth0
$ sudo ip addr add 10.100.0.15/24 dev eth0
```

設定が終わったら、また `inst1` に対して `ping` を実行してみます。

```
$ ping 10.100.0.10
```

うまく動作したでしょうか？先程とは異なり、疎通ができなくなったことが確認できるはずです。これがもし従来のネットワークだった場合、 `10.100.0.0/24` の範囲内のIPアドレスに変更したとしても疎通できますが、OpenVNetはデータベースに従ってより厳格に制限を行うため、`inst2` のIPアドレスが `10.100.0.11` でない限り、通信を許可しません。

=== フローの変化を見る

OpenVNetはOpenFlowで仮想ネットワークをコントロールしていますが、フローエントリを `ovs-ofctl` でそのまま確認するのは大変です。
OpenVNetには `vna` と共にインストールされる `vnflows-monitor` というツールが付属しており、
フロー制御の節で解説したOpenVNetのフローテーブルの分類に基づいて、現在のエッジスイッチのフローエントリを読みやすく整形して表示してくれます。

`vnflows-monitor` を実行するには、OpenVNetが内包するRubyにパスが通っている必要があります。

```
$ PATH=/opt/axsh/openvnet/ruby/bin:${PATH}
```

それでは、 `vnflows-monitor` でフローエントリを表示してみましょう。

```
$ cd /opt/axsh/openvnet/vnet/bin/
$ ./vnflows-monitor
```

エッジスイッチが正しく動作していて、フローエントリが存在する場合、例として以下のような内容が表示されます。

```
(0): TABLE_CLASSIFIER
  0-00        0       0 => SWITCH(0x0)               actions=write_metadata:REMOTE(0x0),goto_table:TABLE_TUNNEL_PORTS(3)
  0-01        0       0 => SWITCH(0x0)              tun_id=0 actions=drop
  0-02       28       0 => PORT(0x1)                in_port=1 actions=write_metadata:TYPE_INTERFACE|LOCAL(0x1),goto_table:TABLE_INTERFACE_EGRESS_CLASSIFIER(15)
  0-02       22       0 => PORT(0x2)                in_port=2 actions=write_metadata:TYPE_INTERFACE|LOCAL(0x5),goto_table:TABLE_INTERFACE_EGRESS_CLASSIFIER(15)
  0-02        0       0 => SWITCH(0x0)              in_port=CONTROLLER actions=write_metadata:LOCAL|NO_CONTROLLER(0x0),goto_table:TABLE_CONTROLLER_PORT(7)
  0-02        0       0 => PORT(0x7ffffffe)         in_port=LOCAL actions=write_metadata:LOCAL(0x0),goto_table:TABLE_LOCAL_PORT(6)
(3): TABLE_TUNNEL_PORTS
  3-00        0       0 => SWITCH(0x0)               actions=drop
(4): TABLE_TUNNEL_NETWORK_IDS
  4-00        0       0 => SWITCH(0x0)               actions=drop
  4-30        0       0 => ROUTE_LINK(0x1)          tun_id=0x10000001,dl_dst=02:00:10:00:00:01 actions=write_metadata:TYPE_ROUTE_LINK(0x1),goto_table:TABLE_ROUTER_CLASSIFIER(33)
  4-30        0       0 => NETWORK(0x1)             tun_id=0x80000001 actions=write_metadata:TYPE_NETWORK(0x1),goto_table:TABLE_NETWORK_SRC_CLASSIFIER(20)
  4-30        0       0 => NETWORK(0x2)             tun_id=0x80000002 actions=write_metadata:TYPE_NETWORK(0x2),goto_table:TABLE_NETWORK_SRC_CLASSIFIER(20)
(6): TABLE_LOCAL_PORT
  6-00        0       0 => SWITCH(0x0)               actions=drop
...
```

このように、フローエントリが (0): TABLE_CLASSIFIER といった形で、OpenVNetのフローテーブルの分類でグループ化されて表示されます。
また、それぞれのフローテーブルの下に表示される行の意味は、左から順に、以下のようになっています。

. フローエントリの優先度に従ったフローテーブルのインデックス
. そのフローエントリにマッチしたパケット数
. フローの `cookie`
. フローの `match`
. フローの `action`

なお、`vnflows-monitor` には、フローの継続的な監視を行う機能もあります。これは `vnflows-monitor` の最も有用な特徴の1つであり、フローエントリの変化がすぐに画面出力に反映されます。この機能を利用するには、 `vnflows-monitor` に以下のような引数を付加して起動します。

```
$ cd /opt/axsh/openvnet/vnet/bin
$ ./vnflows-monitor -d -c 0
```

この方法で起動すると、最初は何も表示されず、パケットが流れるのを待機している状態になります。
この状態で、例として、先程の `inst1` と `inst2` の間で `ping` を実行した時には、次のような内容が出力されます。

```
-------run:4--iteration:43-------
(0): TABLE_CLASSIFIER
  0-02       34       0 => PORT(0x1)                in_port=1 actions=write_metadata:TYPE_INTERFACE|LOCAL(0x1),goto_table:TABLE_INTERFACE_EGRESS_CLASSIFIER(15)
  0-02       28       0 => PORT(0x2)                in_port=2 actions=write_metadata:TYPE_INTERFACE|LOCAL(0x5),goto_table:TABLE_INTERFACE_EGRESS_CLASSIFIER(15)
(15): TABLE_INTERFACE_EGRESS_CLASSIFIER
 15-30       11       0 => INTERFACE(0x1)[0x12]     ip,metadata=TYPE_INTERFACE(0x1),dl_src=10:54:ff:00:00:01,nw_src=10.100.0.10 actions=write_metadata:TYPE_NETWORK(0x1),goto_table:TABLE_INTERFACE_EGRESS_FILTER(18)
 15-30        8       0 => INTERFACE(0x5)[0x12]     ip,metadata=TYPE_INTERFACE(0x5),dl_src=10:54:ff:00:00:02,nw_src=192.168.50.10 actions=write_metadata:TYPE_NETWORK(0x2),goto_table:TABLE_INTERFACE_EGRESS_FILTER(18)
(18): TABLE_INTERFACE_EGRESS_FILTER
 18-00       38       0 => SWITCH(0x0)               actions=goto_table:TABLE_NETWORK_SRC_CLASSIFIER(20)
(20): TABLE_NETWORK_SRC_CLASSIFIER
 20-30       25       0 => NETWORK(0x1)             metadata=TYPE_NETWORK(0x1) actions=goto_table:TABLE_ROUTE_INGRESS_INTERFACE(30)
 20-30       13       0 => NETWORK(0x2)             metadata=TYPE_NETWORK(0x2) actions=goto_table:TABLE_ROUTE_INGRESS_INTERFACE(30)
(30): TABLE_ROUTE_INGRESS_INTERFACE
 30-10        8       0 => INTERFACE(0x6)[0x12]     ip,metadata=TYPE_NETWORK(0x1),dl_dst=02:00:00:00:02:01 actions=write_metadata:TYPE_INTERFACE(0x6),goto_table:TABLE_ROUTE_INGRESS_TRANSLATION(31)
 30-10        8       0 => INTERFACE(0x7)[0x12]     ip,metadata=TYPE_NETWORK(0x2),dl_dst=02:00:00:00:02:02 actions=write_metadata:TYPE_INTERFACE(0x7),goto_table:TABLE_ROUTE_INGRESS_TRANSLATION(31)
(31): TABLE_ROUTE_INGRESS_TRANSLATION
 31-90        8       0 => INTERFACE(0x6)           metadata=TYPE_INTERFACE(0x6) actions=goto_table:TABLE_ROUTER_INGRESS_LOOKUP(32)
 31-90        8       0 => INTERFACE(0x7)           metadata=TYPE_INTERFACE(0x7) actions=goto_table:TABLE_ROUTER_INGRESS_LOOKUP(32)
(32): TABLE_ROUTER_INGRESS_LOOKUP
 32-30        8       0 => ROUTE(0x1)               ip,metadata=TYPE_INTERFACE(0x6),nw_src=10.100.0.0/24 actions=write_metadata:TYPE_ROUTE_LINK|REFLECTION(0x1),goto_table:TABLE_ROUTER_CLASSIFIER(33)
 32-30        8       0 => ROUTE(0x2)               ip,metadata=TYPE_INTERFACE(0x7),nw_src=192.168.50.0/24 actions=write_metadata:TYPE_ROUTE_LINK|REFLECTION(0x1),goto_table:TABLE_ROUTER_CLASSIFIER(33)
(33): TABLE_ROUTER_CLASSIFIER
 33-30       16       0 => ROUTE_LINK(0x1)          metadata=TYPE_ROUTE_LINK(0x1) actions=goto_table:TABLE_ROUTER_EGRESS_LOOKUP(34)
(34): TABLE_ROUTER_EGRESS_LOOKUP
 34-30        8       0 => ROUTE(0x1)               ip,metadata=TYPE_ROUTE_LINK(0x1),nw_dst=10.100.0.0/24 actions=write_metadata:0x8000000600000001,goto_table:TABLE_ROUTE_EGRESS_LOOKUP(35)
 34-30        8       0 => ROUTE(0x2)               ip,metadata=TYPE_ROUTE_LINK(0x1),nw_dst=192.168.50.0/24 actions=write_metadata:0x8000000700000001,goto_table:TABLE_ROUTE_EGRESS_LOOKUP(35)
(35): TABLE_ROUTE_EGRESS_LOOKUP
 35-20        8       0 => INTERFACE(0x6)[0x12]     metadata=VALUE_PAIR(0x8000000600000000/0xffffffff00000000)(0x0) actions=write_metadata:0x702000000000006,goto_table:TABLE_ROUTE_EGRESS_TRANSLATION(36)
 35-20        8       0 => INTERFACE(0x7)[0x12]     metadata=VALUE_PAIR(0x8000000700000000/0xffffffff00000000)(0x0) actions=write_metadata:0x702000000000007,goto_table:TABLE_ROUTE_EGRESS_TRANSLATION(36)
(36): TABLE_ROUTE_EGRESS_TRANSLATION
 36-90        8       0 => INTERFACE(0x6)           metadata=TYPE_INTERFACE(0x6) actions=goto_table:TABLE_ROUTE_EGRESS_INTERFACE(37)
 36-90        8       0 => INTERFACE(0x7)           metadata=TYPE_INTERFACE(0x7) actions=goto_table:TABLE_ROUTE_EGRESS_INTERFACE(37)
(37): TABLE_ROUTE_EGRESS_INTERFACE
 37-20        8       0 => INTERFACE(0x6)[0x12]     metadata=TYPE_INTERFACE(0x6) actions=set_field:02:00:00:00:02:01->eth_src,write_metadata:TYPE_NETWORK(0x1),goto_table:TABLE_ARP_TABLE(40)
 37-20        8       0 => INTERFACE(0x7)[0x12]     metadata=TYPE_INTERFACE(0x7) actions=set_field:02:00:00:00:02:02->eth_src,write_metadata:TYPE_NETWORK(0x2),goto_table:TABLE_ARP_TABLE(40)
(40): TABLE_ARP_TABLE
 40-40        8       0 => INTERFACE(0x1)[0x12]     ip,metadata=TYPE_NETWORK(0x1),nw_dst=10.100.0.10 actions=set_field:10:54:ff:00:00:01->eth_dst,goto_table:TABLE_NETWORK_DST_CLASSIFIER(42)
 40-40        8       0 => INTERFACE(0x5)[0x12]     ip,metadata=TYPE_NETWORK(0x2),nw_dst=192.168.50.10 actions=set_field:10:54:ff:00:00:02->eth_dst,goto_table:TABLE_NETWORK_DST_CLASSIFIER(42)
(42): TABLE_NETWORK_DST_CLASSIFIER
 42-30       25       0 => NETWORK(0x1)             metadata=TYPE_NETWORK(0x1) actions=goto_table:TABLE_NETWORK_DST_MAC_LOOKUP(43)
 42-30       13       0 => NETWORK(0x2)             metadata=TYPE_NETWORK(0x2) actions=goto_table:TABLE_NETWORK_DST_MAC_LOOKUP(43)
(43): TABLE_NETWORK_DST_MAC_LOOKUP
 43-60       12       0 => INTERFACE(0x1)[0x12]     metadata=TYPE_NETWORK(0x1),dl_dst=10:54:ff:00:00:01 actions=write_metadata:TYPE_INTERFACE(0x1),goto_table:TABLE_INTERFACE_INGRESS_FILTER(45)
 43-60        8       0 => INTERFACE(0x5)[0x12]     metadata=TYPE_NETWORK(0x2),dl_dst=10:54:ff:00:00:02 actions=write_metadata:TYPE_INTERFACE(0x5),goto_table:TABLE_INTERFACE_INGRESS_FILTER(45)
(45): TABLE_INTERFACE_INGRESS_FILTER
 45-90       11       0 => INTERFACE(0x1)[0x71]     metadata=TYPE_INTERFACE(0x1) actions=goto_table:TABLE_OUT_PORT_INTERFACE_INGRESS(90)
 45-90        8       0 => INTERFACE(0x5)[0x71]     metadata=TYPE_INTERFACE(0x5) actions=goto_table:TABLE_OUT_PORT_INTERFACE_INGRESS(90)
(90): TABLE_OUT_PORT_INTERFACE_INGRESS
 90-10       12       0 => PORT(0x1)                metadata=TYPE_INTERFACE(0x1) actions=output:1
 90-10        8       0 => PORT(0x2)                metadata=TYPE_INTERFACE(0x5) actions=output:2
```

`inst1` と `inst2` の間でICMP Echo RequestとICMP Echo Replyがエッジスイッチを横断すると、マッチした全てのフローエントリが表示され、マッチしたパケット数のカウンタが増加していきます。
この機能により、パケットがエッジスイッチのどのフローエントリを通過して処理されたかを、一目で知ることができます。また、他の使い方として、例えば `vnctl` で仮想ネットワークを操作した時に、どのようなフローエントリが追加、あるいは削除されたかも確認することができます。

== OpenVNetの活用例

OpenVNetはすでに活用が始まっています。たとえば、京セラコミュニケーションシステムやTIS株式会社にて、OpenFlowの実案件活用や仮想ネットワークの実証実験などを行っており、OpenVNetのテクノロジが活躍しています。

=== IaaS基盤でネットワーク管理をする

WSFでは株式会社あくしゅが筆頭となり、Wakame-vdcと言うIaaS基盤を開発しており、多くのデータセンタで商用利用が始まっています。Wakame-vdcは、データセンタ内部のコンピューティング資源を、動的にマルチテナント化するソフトウェアです。公表可能なものだけでも、すでにいくつもの企業や研究機関で商業化や実用化が進んでいます。

- 国立情報学研究所 (NII): 分散処理の実証実験、クラウド教育教材として活用
- 九州電力: 大規模データの分散処理基盤として
- NTT PCコミュニケーションズ: パブリッククラウド WebARENA VPSクラウド
- 京セラコミュニケーションシステム: パブリッククラウド GreenOffice Unified Cloud
- TIS株式会社: Dockerでの活用、クラウドを跨ったL2延伸の共同実証実験

マルチテナントをするためには、仮想化の技術が重要になります。サーバだけでなく、ネットワークも含めて、仮想化を実現しなければなりません。特に後者について、2012年の始めに、Wakame-vdcはTremaを利用して、仮想ネットワークの技術を内蔵していました。これが後の2013年の秋に分離されて、OpenVNetとしてスピンアウト、独立しました。

=== 分散するDockerを仮想ネットワークで連結する

次に２つ目の活用例として、複数の物理サーバ上に分散するDockerコンテナをOpenVNetを用いた仮想ネットワークで連結する方法をみていきましょう。

これは2015年のはじめに、TIS株式会社が仮想ネットワークとコンテナ技術の実験を行った事例となります。

=== Dockerのネットワーク

Docker footnote:[Dockerの詳細は、Dockerの公式ドキュメント(https://docs.docker.com/)を参照ください] とは、dotCloud社（現Docker社）が自社のパブリックPaaSを実現するために開発した技術をOSS化したものです。アプリケーションの実行環境を容易に素早く、かつ他の影響を受けないようにして立ち上げるために、Dockerは様々なLinuxの技術を用いて「他から隔離された環境（＝コンテナ） 」を作り出します。

Dockerは様々なリソースを隔離しますが、ネットワークもその隔離すべきリソースの一つです。そのためDockerは、ネットワークネームスペースや仮想ネットワークインタフェース等の技術を用いて、Linuxサーバ上に他から隔離された内部ネットワークを構成します。ただしそのままではサーバの外部と通信ができませんので、Dockerは通常、iptablesの機能を用いて外部ネットワークと連携できるようにします。

[[docker_network]]
image::docker_network.png[caption="図17-1",title="Dockerのネットワーク"]

単独のサーバ内でDockerを利用するだけならこの方式で良いのですが、複数のサーバでDockerを動作させたい場合には問題が生じます。Dockerコンテナが所属するネットワークはサーバ内に閉じていますので、異なるサーバで動作しているDockerコンテナ同士が、そのDockerコンテナに付与されたIPアドレスで通信することができないのです。

この問題を解決するために様々なOSSのDockerネットワーキングツールが公開されていますし、2015年10月にリリースされたDocker v1.9からは、Docker自身が複数サーバを跨った仮想ネットワークを構成できるようになりました。

しかしこの実験を行った時点ではまだ、Docker自身は仮想ネットワークを構成する機能を持っていませんでした。また当時公開されていたOSSツールには、セキュリティグループのようなOpenVNetが持つ高度なネットワーク機能がありませんでした。そこで本実験では、OpenVNetを用いて敷設した仮想ネットワークにDockerコンテナを所属させることで、サーバを跨ったDockerコンテナ間がシームレスに通信できること、及びDockerのネットワークにセキュリティグループのような高度なネットワーク機能を付与できることを確認しました。

=== Docker+OpenVNet

同一の物理ネットワークに接続したサーバ2台と、普通のルータを挟んで別の物理ネットワークに接続したサーバ1台の、合計3つのサーバ上でDockerコンテナを動作させ、それらをOpenVNetを用いて敷設した仮想ネットワークに所属させてみましょう。

まず最初に、各サーバ上にエッジスイッチを立ち上げます。各エッジスイッチのDatapath IDが重複しないように注意してください。

次に各サーバ上でDockerコンテナを立ち上げ、Dockerコンテナを各サーバ内部に立ち上げたエッジスイッチに接続します。この際、後からOpenVNetに設定できるように、Dockerコンテナ側のvethのMACアドレスとIPアドレスを明示的に指定しておきます。

さらに以下の手順でOpenVNetを設定します。

* 各エッジスイッチのDatapath IDをOpenVNetに指定する
* 各サーバが所属する物理ネットワークの情報をOpenVNetに指定する
* OpenVNetが敷設する仮想ネットワークを定義する
* 各サーバの物理ネットワークインタフェースの情報をOpenVNetに指定する
* 立ち上げたDockerコンテナの仮想ネットワークインタフェースの情報をOpenVNetに指定する
* OpenVNetが制御するセキュリティグループを定義する
* 各仮想ネットワークインタフェースに望みのセキュリティグループを割り当てる
* OpenVNet上に仮想ルータを構成して、物理ネットワークと仮想ネットワーク間のルーティングを定義する

最後に各サーバとDockerコンテナにスタティックルートを設定すれば、OpenVNetを用いたDockerネットワーキングが完成します。

各サーバ上のDockerコンテナは、OpenVNetが敷設した同じ仮想ネットワークに所属していますので、異なるサーバのDockerコンテナへそのIPアドレスを用いて通信することが可能となります。またセキュリティグループの設定に従い、到達すべきでないパケットはOpenVNetがDROPするため、個々のDockerコンテナにパケットフィルタルールを定義する必要が無くなります。

image::docker_openvnet_1.png[caption="図17-2",title="OpenVNetを用いたDockerネットワーキング"]

なお、ここで説明した手順を実際に実行し動作させるツールキットを、walfisch footnote:[https://github.com/tech-sketch/walfisch] というオープンソースソフトウェアとして公開しています。実際に実行したコマンドが標準出力に表示されますので、OpenVNetを用いたDockerネットワーキングに興味がある方は一度動作させてみると良いでしょう。

=== 分散するデータセンタ間を仮想ネットワークで連結する

最後に、 複数のデータセンタ間を跨って任意の仮想ネットワークを構成する例を見てみましょう。
この例は、2014年度にTIS株式会社と株式会社あくしゅが協力し、各IaaSやオンプレミスのネットワーク機能に依存しないネットワーク制御について、OpenVNetを活用して共同検証を行ったものです。

現存するパブリックIaaSの持つネットワーク機能は、それぞれ大きく利用方法や特徴が異なっています。このため、パブリックIaaSの利用者はそれらに強く依存したシステム設計を行う必要があります。しかし、OpenVNetを利用することで、パブリックIaaSのネットワーク機能に依存せず、複数のパブリックあるいはプライベートIaaSに跨った仮想的なネットワークを構成することが可能となるため、IaaS間の段階的なシステム移行の実現性を高めることができます。

=== プライベートIaaSとパブリックIaaSの連結例

それでは、プライベートIaaSとパブリックIaaSのネットワークを連結する構成例をみてみましょう。

OpenVNetは独立して動作することができますが、本来は仮想データセンタを構築するOSSであるwakame-vdc footnote:[https://github.com/axsh/wakame-vdc] のネットワーク機能としてスピンアウトしたソフトウェアであるため、プライベートIaaSとしてwakame-vdc、パブリックIaaSとしてAmazon Web Servicesを利用するケースを想定します。

[[narukozaka_tools]]
image::narukozaka_tools.png[caption="図17-4",title="プライベートIaaSとパブリックIaaSの連結構成"]

OpenVNetは、フローによってOpenVNetの仮想ネットワークと外部のネットワークの間をシームレスに接続するVNetEdge機能を持っています。

この構成例では、仮想ネットワークIDとVLAN IDの変換規則をOpenVNetに登録しておくことで、wakame-vdcの仮想ネットワークと、Amazon Web ServicesのVirtual Private Cloudで構築されたネットワークの間を流れるパケットがVNetEdgeのOpen vSwitchを通過する際に、この２つのネットワークが同一のネットワークであるかのようにパケット転送を制御します。

このツールキットはOSSとして公開しており footnote:[https://github.com/cloudconductor-incubator/narukozaka-tools]、この他にも多くの機能を持ちます。

* IaaSのインスタンスイメージの作成と起動
* IaaSのインスタンスにインストールするミドルウェアの自動設定
* IaaSのネットワーク上に、VNetEdgeをスイッチとしたスター型のネットワークトポロジを構築する機能
* wakame-vdcとパブリックIaaSの間を自動的に連結する機能

またセキュリティの案件に応じ、wakame-vdc側のインスタンスとIaaS側のVNetEdge間の通信を暗号化するといった、柔軟な対応も可能です。

== なぜTremaを採用したのか

// TODO ここは少し削りましょう。本全体のトーンとして、Trema開発者による解説という視点で統一書いています。以下の部分だけユーザ視点が混ざっているので、Trema開発者側の視点に書き直す必要があります。

Tremaの最大の魅力は、数々あるのですが、大まかにまとめると以下の通りです。

1. 優れた設計がある: フレームワークとして最小限のコードで最大限の効果を得られる
2. コミュニティが機能している: コードを評価でき、貢献が適切に反映されている
3. 言語の親和性がある: OpenVNetはTremaと同じRubyで組まれている

まず、OpenVNetがOpenFlowを使っていく方針を出した際、様々なツールキットやフレームワークが出ており、いくつか調査をしました。その中で、Tremaは当初より、利用する側から見た設計が、非常に合理的で洗練されており、やりたい事に対していつでも最短のコードで目的に辿り着けるようになっていました。

また、今でこそ十分な機能がありますが、当初はまだ機能が足りない部分もあり、そこはコードをOpenVNet側からコミットして貢献することもできました。オンラインだけでなく、オフラインのコミュニティもOpenVNetのプロジェクトからは魅力的でした。次に書こうとしているコードの相談などもその場で可能なのです。コードは双方にとって有益であれば採用され、お互いにソフトウェアとして成長していくことができ、まさにバザール式の開発が機能しています。

Tremaは、OpenVNetにとって、あらゆる面から大きなアドバンテージのある選択でした。ソフトウェアは、それを使う人が育てていくことで、より良い物になっていきます。貢献の仕方は様々です。一番簡単なところでは、下記のような方法があるでしょう。

- GitHub上でWatchやForkをしてみる
- コードをダウンロードして使ってみる
- 思うところや、成果をブログを書いて公表してみる
- 既存ドキュメントの英訳や日本語訳をする
- 足りないドキュメントがあれば加筆や新規執筆をする
- GitHubのIssue機能を通じて、バグ報告をしてみる
- バグを修正するパッチの送付をしてみる
- 機能の追加をして提案をしてみる

上記を例に、自分の能力にあった貢献の仕方で、Tremaの世界を共に大きくしていくことができます。この素晴らしい取り組みに、ぜひ皆様ご参加ください。お待ちしております。
